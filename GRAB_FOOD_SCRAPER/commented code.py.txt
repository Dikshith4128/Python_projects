# Import necessary libraries
from seleniumwire import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.keys import Keys
import json
import gzip
import time
import logging

# Define the RestaurantScraper class
class RestaurantScraper:
    def __init__(self, location_to_search):
        # Initialize class attributes
        self.location_to_search = location_to_search # Store the location to be searched
        # Create the output file name based on the location (replace spaces with underscores)
        self.output_file = f'{location_to_search.replace(" ", "_")}_output_latlng.ndjson'
        self.driver = None# Initialize the WebDriver as None
        # Explanation:
        # - The __init__ method is a special method in Python classes that is called when               an object is created.
        # - It initializes the attributes of the class, setting the initial state of the object.
        # - `location_to_search` is a parameter that takes the location value when an object of RestaurantScraper is created.
        # - `output_file` is generated based on the provided location, replacing spaces with underscores and adding a suffix.
        # - `driver` is initialized as None and will be set later when the WebDriver is initialized.


# Section for initializing the Chrome WebDrive
    def _initialize_driver(self):
        # Create Chrome options
        chrome_options = Options()

        # Add options to handle certificate errors and SSL errors
        chrome_options.add_argument('--ignore-certificate-errors')
        chrome_options.add_argument('--ignore-ssl-errors')

        # Configure proxy settings to go direct, bypassing the proxy
        chrome_options.add_argument('--proxy-server="direct://"')
        chrome_options.add_argument('--proxy-bypass-list=*')

        # Disable web security to handle SSL verification issues
        chrome_options.add_argument('--disable-web-security')

        # Specify the path to the Selenium Wire CA certificate
        chrome_options.add_argument(r'--ca-certificate=C:\Users\diksh\OneDrive\Desktop\grab_foods_restuarent_data\ca.crt')

        # Create a custom user data profile with SSL verification disabled
        chrome_options.add_argument(r'--user-data-dir=C:\Users\diksh\OneDrive\Desktop\grab_foods_restuarent_data\custom_chrome_profile')

        # Initialize the Chrome WebDriver with specified options 
        self.driver = webdriver.Chrome(seleniumwire_options={'disable_certificate_verification': True}, options=chrome_options)

        # Explanation:
        # - This method is responsible for setting up and initializing the Chrome WebDriver with specific options.
        # - `chrome_options` is an object that holds various configurations for the Chrome browser.
        # - Arguments are added to the options to handle certificate errors, SSL errors, and proxy settings.
        # - Web security is disabled to handle SSL verification issues.
        # - The path to the CA certificate and user data directory is specified for SSL verification and custom profiles.
        # - The Chrome WebDriver is initialized using the configured options and assigned to the class attribute `self.driver`.



# Section for searching the location using the WebDriver
    def _search_location(self):
        # Find the search bar element using its ID attribute
        search_bar = self.driver.find_element(By.ID, "location-input")

        # Find the submit button element using a CSS selector
        submit_button = self.driver.find_element(By.CSS_SELECTOR, '.ant-btn.submitBtn___2roqB.ant-btn-primary')

        # Check if the search bar element is found
        if search_bar:


            # Clear the search bar to ensure it's empty
            search_bar.clear()

            # Type the location into the search bar
            search_bar.send_keys(self.location_to_search)

            # Wait for 5 seconds (adjust this based on the page loading time)
            time.sleep(5)  

            # Press the Enter key to submit the search
            search_bar.send_keys(Keys.ENTER)

            # Wait for 5 seconds after submitting the search (adjust this based on the page loading time)
            time.sleep(5)

            # Click the submit button to initiate the search (additional wait time)
            submit_button.click()
            time.sleep(5)

# Section for gradually scrolling to the bottom of the page
    def _slow_scroll_to_bottom(self):
        # Record the start time for measuring the elapsed time
        start_time = time.time()

        # Loop until 30 seconds have passed
        while time.time() - start_time < 30:
            # Execute JavaScript to scroll the window by 400 pixels vertically
            self.driver.execute_script("window.scrollBy(0, 400);")

            # Introduce a short delay of 0.5 seconds (adjust this based on the desired scrolling speed)
            time.sleep(0.5)
            # Explanation:
            # - This method is responsible for gradually scrolling to the bottom of the webpage.
            # - The starting time is recorded to measure the elapsed time.
            # - The loop continues until 30 seconds have passed.
            # - Within the loop, JavaScript is executed to scroll the window by 400 pixels vertically.
            # - A short delay of 0.5 seconds is introduced to control the scrolling speed.
            # - This process continues until the elapsed time reaches 30 seconds.
            # - The purpose is to simulate a gradual scroll down the page, allowing content to load dynamically.
            # - Adjust the duration and scrolling increments based on the page's characteristics and loading speed.


# Section for scraping restaurant data from the loaded page
    def _scrape_restaurant_data(self):
        restaurant_list = []# Initialize an empty list to store scraped restaurant data
        # Scroll to the bottom of the page to load more content
        self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(5)# Allow time for the new content to load
        offset = 0# Initialize an offset for tracking the number of requests

        # Continue the loop until a sufficient number of requests are detected
        while True:
            # Filter and retrieve requests related to restaurant data
            requests = [request for request in self.driver.requests if request.response and request.url.startswith('https://portal.grab.com/foodweb/v2/search')]

            # Check if the number of requests is greater than or equal to 8
            if len(requests) >= 8:
                break
        # Iterate over the filtered requests to extract restaurant data
        for request in requests:
            res = request.response
            if res.headers.get('Content-Encoding', '') == 'gzip':
                # Decompress and decode the gzip response body
                body_json = json.loads(gzip.decompress(res.body).decode('utf-8'))
            else:
                # Decode the response body if not gzipped
                body_json = json.loads(res.body.decode('utf-8'))
            
            # Extract restaurant information from the JSON response
            search_merchants = body_json.get('searchResult', {}).get('searchMerchants', [])
            for restaurant in search_merchants:
                try:
                    _data = {} # Create a dictionary to store individual restaurant data
                    # Extract relevant data from the restaurant object
                    _data['Restaurant ID'] = restaurant.get('id', '')
                    _data['Restaurant Name'] = restaurant.get('address', {}).get('name', '')
                    _data['Restaurant latitude '] = restaurant.get('latlng', {}).get('latitude', '')
                    _data['Restaurant longitude'] = restaurant.get('latlng', {}).get('longitude', '')
                    _data['Estimate time of Delivery'] = restaurant.get('estimatedDeliveryTime', '')
                    _data['Restaurant Cuisine'] = restaurant.get('merchantBrief', {}).get('cuisine', '')
                    _data['Restaurant Notice'] = restaurant.get('merchantBrief', {}).get('openHours', '')
                    _data['Image Link of the Restaurant'] = restaurant.get('merchantBrief', {}).get('photoHref', '')
                    _data['Restaurant Distance from Delivery Location'] = restaurant.get('merchantBrief', {}).get('distanceInKm', '')
                    _data['Restaurant Rating'] = restaurant.get('merchantBrief', {}).get('rating', '')
                    promo_info = restaurant.get('merchantBrief', {}).get('promo', {})
                    _data['Is promo available'] = promo_info.get('hasPromo', False)
                    _data['Promotional Offers Listed for the Restaurant'] = promo_info.get('description', 'None') if _data['Is promo available'] else 'None'
                     # Round the estimated delivery fee to 3 decimal places
                    _data['estimateDeliveryFee'] = round(float(_data['Restaurant Distance from Delivery Location']) * 3, 3) if _data['Restaurant Distance from Delivery Location'] else None
                    # Append the restaurant data dictionary to the list
                    restaurant_list.append(_data)
                except Exception as e:
                    # Handle exceptions and log an error message
                    logging.exception(f"Error processing restaurant data: {str(e)}")
        return restaurant_list
        # Explanation:
        # - This method is responsible for extracting restaurant data from the loaded page.
        # - It initiates a scroll to the bottom of the page to trigger the loading of additional content.
        # - The loop continues until a sufficient number of requests related to restaurant data are detected.
        # - Requests are filtered based on their URL, targeting the endpoint for restaurant search.
        # - The response body is decoded, and restaurant information is extracted from the JSON response.
        # - Individual restaurant data is stored in a dictionary (_data) and appended to the restaurant_list.
        # - The estimated delivery fee is rounded to 3 decimal places.
        # - Any exceptions during the data extraction process are logged for error handling.



# Section for performing quality control (QC) on the extracted data
    def _perform_qc(self, restaurant_list):
        for restaurant in restaurant_list:
            # Check if the 'Restaurant ID' key is missing in the extracted data for a restaurant
            if not restaurant.get('Restaurant ID'):
                # Log a warning message indicating the missing 'Restaurant ID'
                logging.warning("Missing Restaurant ID in extracted data.")
                # Explanation:
                # - This method is responsible for performing quality control (QC) checks on the extracted restaurant data.
                # - It iterates through each restaurant in the provided restaurant_list.
                # - For each restaurant, it checks if the 'Restaurant ID' key is present in the extracted data.
                # - If the 'Restaurant ID' is missing, it logs a warning message using the logging module.
                # - Quality control checks are essential to ensure the integrity and completeness of the extracted data.
                # - Adjust the QC checks based on specific requirements and the structure of the extracted data.


# Section for saving data to a gzipped NDJSON file
    def _save_to_ndjson_gz(self, restaurant_list):
        # Create the filename for the gzipped NDJSON file
        gzipped_file = f'{self.output_file}.gz'

        # Use gzip module to open the file in write mode, text mode, and utf-8 encoding
        with gzip.open(gzipped_file, 'wt', encoding='utf-8') as f:
            # Iterate through each restaurant in the restaurant_list
            for restaurant in restaurant_list:
                # Dump the restaurant data as a JSON string to the gzipped file
                json.dump(restaurant, f, ensure_ascii=False)
                # Write a newline character to separate each JSON object
                f.write('\n')
         
        # Print a confirmation message indicating the successful file creation
        print(f'\nDone. Output saved to {gzipped_file}')
        # Explanation:
        # - This method is responsible for saving the restaurant data to a gzipped NDJSON (Newline Delimited JSON) file.
        # - The filename is generated by appending '.gz' to the original output file name.
        # - The `gzip.open` method is used to open the file in write mode ('wt') with utf-8 encoding.
        # - The `ensure_ascii` parameter is set to False to allow non-ASCII characters in the JSON output.
        # - The method iterates through each restaurant in the restaurant_list, dumps the JSON data, and writes a newline character.
        # - The result is a gzipped NDJSON file with each line containing a JSON object.
        # - A confirmation message is printed to indicate the successful file creation.
        # - NDJSON is a convenient format for storing multiple JSON objects in a single file, with each object on a new line.



# Main function to run the scraper
    def run_scraper(self):
        try:
            # Initialize the WebDriver
            self._initialize_driver()

            # Load the Grab Food websit
            self.driver.get("https://food.grab.com/sg/en")

            # Search for the specified location
            self._search_location()

            # Gradually scroll to the bottom of the page
            self._slow_scroll_to_bottom()

            # Scrape restaurant data from the loaded pag
            restaurant_list = self._scrape_restaurant_data()

            # Perform quality control (QC) on the extracted data
            self._perform_qc(restaurant_list)

            # Save data to a gzipped NDJSON file
            self._save_to_ndjson_gz(restaurant_list)
        except Exception as e:
            # Log an exception message if an error occurs during execution
            logging.exception(f"An error occurred: {str(e)}")
        finally:
            # Close the WebDriver in the 'finally' block to ensure proper cleanup
            if self.driver:
                self.driver.quit()

# Main function to initiate the scraper for specified locations
def main():
    # Specify locations to search for restaurants
    locations_to_search = ["Ang Mo Kio Avenue 10, #01-1574, Singapore, 560456", "Choa Chu Kang North 6, Singapore, 689577"]

    # Iterate over the specified locations
    for location in locations_to_search:
        # Create an instance of the RestaurantScraper class for each location
        scraper_instance = RestaurantScraper(location)

        # Run the scraper for the current location
        scraper_instance.run_scraper()
        
# Run the main function if the script is executed directl
if __name__ == "__main__":
    main()

# Explanation:
# - The `run_scraper` function orchestrates the entire scraping process.
# - It initializes the WebDriver, loads the Grab Food website, searches for the specified location,
#   scrolls to the bottom of the page, scrapes restaurant data, performs quality control, and saves the data.
# - Exception handling is implemented to log any errors that may occur during the execution.
# - The WebDriver is closed in the 'finally' block to ensure proper cleanup even in case of exceptions.
# - The `main` function serves as the entry point for the script.
# - It specifies the locations to search for restaurants and iterates over each location,
#   creating and running a scraper instance for each location.
# - This structure allows the script to be executed for multiple locations in a modular way.
